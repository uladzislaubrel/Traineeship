{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c48899-91a2-4236-b94c-ab75fffe47d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ce38c76-2dfd-4377-9e18-be24457f41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44babbaf-6c27-4aba-9b9f-ed516d1fc14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1448bd5e-4de3-405a-9851-539b739069de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:4040\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyOldProject\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", r\"C:\\spark-3.5.7-bin-hadoop3\\jars\\postgresql-42.7.8.jar\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Подключение к Postgres в Docker\n",
    "url = \"jdbc:postgresql://localhost:5433/postgres\"\n",
    "properties = {\"user\": \"postgres\", \"password\": \"123456\", \"driver\": \"org.postgresql.Driver\"}\n",
    "print(spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5358d15c-9257-42d1-9a88-97f7c664b70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       name|count|\n",
      "+-----------+-----+\n",
      "|      Drama|  152|\n",
      "|      Music|  152|\n",
      "|     Travel|  151|\n",
      "|    Foreign|  150|\n",
      "|      Games|  150|\n",
      "|   Children|  150|\n",
      "|     Action|  149|\n",
      "|     Sci-Fi|  149|\n",
      "|  Animation|  148|\n",
      "|     Family|  147|\n",
      "|   Classics|  147|\n",
      "|        New|  147|\n",
      "|     Sports|  145|\n",
      "|Documentary|  145|\n",
      "|     Comedy|  143|\n",
      "|     Horror|  142|\n",
      "+-----------+-----+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#3905L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(count#3905L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=18068]\n",
      "      +- HashAggregate(keys=[name#3883], functions=[count(1)])\n",
      "         +- Exchange hashpartitioning(name#3883, 200), ENSURE_REQUIREMENTS, [plan_id=18065]\n",
      "            +- HashAggregate(keys=[name#3883], functions=[partial_count(1)])\n",
      "               +- Project [name#3883]\n",
      "                  +- SortMergeJoin [category_id#3882], [category_id#3889], Inner\n",
      "                     :- Sort [category_id#3882 ASC NULLS FIRST], false, 0\n",
      "                     :  +- Exchange hashpartitioning(category_id#3882, 200), ENSURE_REQUIREMENTS, [plan_id=18057]\n",
      "                     :     +- Scan JDBCRelation(category) [numPartitions=1] [category_id#3882,name#3883] PushedFilters: [*IsNotNull(category_id)], ReadSchema: struct<category_id:int,name:string>\n",
      "                     +- Sort [category_id#3889 ASC NULLS FIRST], false, 0\n",
      "                        +- Exchange hashpartitioning(category_id#3889, 200), ENSURE_REQUIREMENTS, [plan_id=18058]\n",
      "                           +- Scan JDBCRelation(film_category) [numPartitions=1] [category_id#3889] PushedFilters: [*IsNotNull(category_id)], ReadSchema: struct<category_id:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query 1\n",
    "df_category = spark.read.jdbc(url=url, table=\"category\", properties=properties)\n",
    "df_film_category = spark.read.jdbc(url=url, table=\"film_category\", properties=properties)\n",
    "df_join = df_category.join(df_film_category, on = \"category_id\", how = \"inner\")\n",
    "df_group = df_join.groupBy(\"name\").count()\n",
    "df_order = df_group.orderBy(\"count\", ascending = False)\n",
    "df_order.show()\n",
    "df_order.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "400ff2c7-5ff5-46c3-bfbf-354b145bfff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|             actor|count|\n",
      "+------------------+-----+\n",
      "|       SUSAN DAVIS|  825|\n",
      "|    GINA DEGENERES|  753|\n",
      "|    MATTHEW CARREY|  678|\n",
      "|       MARY KEITEL|  674|\n",
      "|ANGELA WITHERSPOON|  654|\n",
      "|       WALTER TORN|  640|\n",
      "|       HENRY BERRY|  612|\n",
      "|       JAYNE NOLTE|  611|\n",
      "|        VAL BOLGER|  605|\n",
      "|     SANDRA KILMER|  604|\n",
      "+------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query 2\n",
    "df_rental = spark.read.jdbc(url=url, table=\"rental\", properties=properties)\n",
    "df_inventory = spark.read.jdbc(url=url, table=\"inventory\", properties=properties)\n",
    "df_film = spark.read.jdbc(url=url, table=\"film\", properties=properties)\n",
    "df_film_actor = spark.read.jdbc(url=url, table=\"film_actor\", properties=properties)\n",
    "df_actor = spark.read.jdbc(url=url, table=\"actor\", properties=properties)\n",
    "df_actor1 = df_actor.withColumn(\"actor\", F.concat(F.col(\"first_name\"), F.lit(\" \"), F.col(\"last_name\")))\n",
    "df_join = df_rental.join(df_inventory, \"inventory_id\").join(df_film, \"film_id\").join(df_film_actor, \"film_id\").join(df_actor1, \"actor_id\")\n",
    "df_group = df_join.groupBy(\"actor\").count()\n",
    "df_order = df_group.orderBy(\"count\", ascending = False)\n",
    "df_order.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef19fa6c-12cd-4d3d-ac14-946cb7982bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|   name|     sum|\n",
      "+-------+--------+\n",
      "|Foreign|10507.67|\n",
      "+-------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query 3\n",
    "df_payment = spark.read.jdbc(url=url, table=\"payment\", properties=properties)\n",
    "df_join = df_category.join(df_film_category, \"category_id\").join(df_inventory, \"film_id\").join(df_rental, \"inventory_id\").join(df_payment, \"rental_id\")\n",
    "df_group = df_join.groupBy(\"name\").agg(F.sum(\"amount\").alias(\"sum\"))\n",
    "df_order = df_group.orderBy(\"sum\", ascending = False)\n",
    "df_order.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "218bf391-9dcb-4040-9320-7a4cc738a791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               title|\n",
      "+--------------------+\n",
      "|      CHOCOLATE DUCK|\n",
      "|       BUTCH PANTHER|\n",
      "|        VOLUME HOUSE|\n",
      "|      ORDER BETRAYED|\n",
      "|        TADPOLE PARK|\n",
      "|    KILL BROTHERHOOD|\n",
      "|FRANKENSTEIN STRA...|\n",
      "|    CROSSING DIVORCE|\n",
      "|    SUICIDES SILENCE|\n",
      "|       CATCH AMISTAD|\n",
      "|     PERDITION FARGO|\n",
      "|       FLOATS GARDEN|\n",
      "|           GUMP DATE|\n",
      "|        WALLS ARTIST|\n",
      "|  GLADIATOR WESTWARD|\n",
      "|         HOCUS FRIDA|\n",
      "|ARSENIC INDEPENDENCE|\n",
      "|         MUPPET MILE|\n",
      "|   FIREHOUSE VIETNAM|\n",
      "|       ROOF CHAMPION|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query 4\n",
    "df_anti_join = df_film.join(df_inventory, on = \"film_id\", how = \"leftanti\")\n",
    "df_anti_join.select(\"title\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8bad63e-b338-4b1c-885f-994e953f27df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+----------+\n",
      "|           actor|film_count|dense_rank|\n",
      "+----------------+----------+----------+\n",
      "|    SIDNEY CROWE|         9|         1|\n",
      "|    RICHARD PENN|         9|         1|\n",
      "|    EWAN GOODING|         9|         1|\n",
      "|      DAN HARRIS|         8|         2|\n",
      "|       KIM ALLEN|         8|         2|\n",
      "|      ALEC WAYNE|         8|         2|\n",
      "|      MARY TANDY|         8|         2|\n",
      "|    JANE JACKMAN|         8|         2|\n",
      "|  RUSSELL TEMPLE|         8|         2|\n",
      "|    SPENCER PECK|         8|         2|\n",
      "|  MATTHEW CARREY|         8|         2|\n",
      "|     SUSAN DAVIS|         8|         2|\n",
      "|      JADA RYDER|         8|         2|\n",
      "|   ANGELA HUDSON|         7|         3|\n",
      "|    WARREN NOLTE|         7|         3|\n",
      "|MINNIE ZELLWEGER|         7|         3|\n",
      "|     GENE WILLIS|         7|         3|\n",
      "|  AUDREY OLIVIER|         7|         3|\n",
      "|  JULIANNE DENCH|         7|         3|\n",
      "|      JAMES PITT|         7|         3|\n",
      "|    KENNETH TORN|         7|         3|\n",
      "|  DARYL WAHLBERG|         7|         3|\n",
      "|    HELEN VOIGHT|         7|         3|\n",
      "+----------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query 5\n",
    "df_category = spark.read.jdbc(url=url, table=\"category\", properties=properties)\n",
    "df_film_category = spark.read.jdbc(url=url, table=\"film_category\", properties=properties)\n",
    "df_film = spark.read.jdbc(url=url, table=\"film\", properties=properties)\n",
    "df_film_actor = spark.read.jdbc(url=url, table=\"film_actor\", properties=properties)\n",
    "df_actor = spark.read.jdbc(url=url, table=\"actor\", properties=properties)\n",
    "df_join = df_category.join(df_film_category, \"category_id\").join(df_film, \"film_id\"). join(df_film_actor, \"film_id\").join(df_actor, \"actor_id\")\n",
    "df_filter = df_join.filter(F.col(\"name\") ==\"Children\")\n",
    "df_edit_column = df_filter.withColumn(\"actor\", F.concat_ws(\" \", \"first_name\", \"last_name\"))\n",
    "df_group = df_edit_column.groupBy(\"actor\").agg(F.count(\"film_id\").alias(\"film_count\"))\n",
    "df_order = df_group.orderBy(\"film_count\", ascending = False)\n",
    "\n",
    "window = Window.partitionBy().orderBy(F.desc(\"film_count\"))\n",
    "df_window = df_order.withColumn(\"dense_rank\", F.dense_rank().over(window))\n",
    "df_top3 = df_window.filter(F.col(\"dense_rank\") <= 3)\n",
    "df_top3.show(30)\n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd83cb59-5f2b-4399-905a-c9144de7ced0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+------------+\n",
      "|              city|active_cnt|inactive_cnt|\n",
      "+------------------+----------+------------+\n",
      "|          Uluberia|         0|           1|\n",
      "|         Najafabad|         0|           1|\n",
      "|         Pingxiang|         0|           1|\n",
      "|          Xiangfan|         0|           1|\n",
      "|        Kumbakonam|         0|           1|\n",
      "|       Szkesfehrvr|         0|           1|\n",
      "|  Charlotte Amalie|         0|           1|\n",
      "|            Kamyin|         0|           1|\n",
      "|            Daxian|         0|           1|\n",
      "|     Coatzacoalcos|         0|           1|\n",
      "|           Wroclaw|         0|           1|\n",
      "|            Ktahya|         0|           1|\n",
      "|            Amroha|         0|           1|\n",
      "|   Southend-on-Sea|         0|           1|\n",
      "|           Bat Yam|         0|           1|\n",
      "|          Fengshan|         1|           0|\n",
      "|A Corua (La Corua)|         1|           0|\n",
      "|           El Alto|         1|           0|\n",
      "|              Linz|         1|           0|\n",
      "|          Myingyan|         1|           0|\n",
      "+------------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query 6\n",
    "df_customer = spark.read.jdbc(url=url, table=\"customer\", properties=properties)\n",
    "df_city = spark.read.jdbc(url=url, table=\"city\", properties=properties)\n",
    "df_address = spark.read.jdbc(url=url, table=\"address\", properties=properties)\n",
    "df_join = df_customer.join(df_address,\"address_id\").join(df_city, \"city_id\")\n",
    "df_group = df_join.groupBy(\"city\").agg(F.sum(F.when(F.col(\"active\")==1, 1).otherwise(0)).alias(\"active_cnt\"), \\\n",
    "                                       F.sum(F.when(F.col(\"active\")==0, 1).otherwise(0)).alias(\"inactive_cnt\"))\n",
    "df_order = df_group.orderBy(\"inactive_cnt\", ascending = False) \n",
    "df_order.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e6d2afd-9afa-4e4b-ac64-fcaafa88c682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------------------+----+\n",
      "|                city|category|             hours|rank|\n",
      "+--------------------+--------+------------------+----+\n",
      "|  A Corua (La Corua)|  Sci-Fi| 931.9166666666666|   1|\n",
      "|                Abha|  Action|1184.9166666666667|   1|\n",
      "|           Abu Dhabi|  Family| 920.2166666666667|   1|\n",
      "|                Acua|  Action|1015.3333333333334|   1|\n",
      "|               Adana|Classics|1012.8666666666667|   1|\n",
      "|         Addis Abeba|   Music| 904.9666666666666|   1|\n",
      "|                Aden|  Sports|1121.3166666666668|   1|\n",
      "|               Adoni|  Comedy| 891.4000000000001|   1|\n",
      "|          Ahmadnagar|  Family| 943.2666666666667|   1|\n",
      "|            Akishima|Children|1430.7666666666667|   1|\n",
      "|               Akron|  Family|1035.8333333333333|   1|\n",
      "|         Alessandria|Children| 756.3666666666666|   1|\n",
      "|Allappuzha (Allep...|Classics|             846.4|   1|\n",
      "|             Allende|Classics|            1004.2|   1|\n",
      "|     Almirante Brown| Foreign|1054.8333333333333|   1|\n",
      "|            Alvorada|Classics| 945.8333333333335|   1|\n",
      "|            Ambattur|   Games| 980.3333333333334|   1|\n",
      "|          Amersfoort|  Action| 633.6666666666667|   1|\n",
      "|              Amroha|   Drama| 684.7833333333334|   1|\n",
      "|      Angra dos Reis|  Family| 821.9333333333333|   1|\n",
      "+--------------------+--------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query 7\n",
    "df_rental = spark.read.jdbc(url=url, table=\"rental\", properties=properties)\n",
    "df_customer = spark.read.jdbc(url=url, table=\"customer\", properties=properties)\n",
    "df_address = spark.read.jdbc(url=url, table=\"address\", properties=properties)\n",
    "df_city = spark.read.jdbc(url=url, table=\"city\", properties=properties)\n",
    "df_inventory = spark.read.jdbc(url=url, table=\"inventory\", properties=properties)\n",
    "df_film = spark.read.jdbc(url=url, table=\"film\", properties=properties)\n",
    "df_film_category = spark.read.jdbc(url=url, table=\"film_category\", properties=properties)\n",
    "df_category = spark.read.jdbc(url=url, table=\"category\", properties=properties)\n",
    "df_join = df_rental.join(df_customer, \"customer_id\" \\\n",
    "                        ).join(df_address, \"address_id\" \\\n",
    "                              ).join(df_city, \"city_id\" \\\n",
    "                                ).join(df_inventory, \"inventory_id\" \\\n",
    "                                      ).join(df_film, \"film_id\" \\\n",
    "                                            ).join(df_film_category, \"film_id\" \\\n",
    "                                                  ).join(df_category, \"category_id\")\n",
    "df_hours = df_join.withColumn(\"diff_hours\", \n",
    "    (F.unix_timestamp(\"return_date\") - F.unix_timestamp(\"rental_date\")) / 3600\n",
    ")\n",
    "\n",
    "df_group = df_hours.groupBy(\"city\", \"name\").agg(F.sum(\"diff_hours\").alias(\"hours\"))\n",
    "df_filter = df_group.filter(F.col(\"city\").ilike(\"A%\") | F.col(\"city\").ilike(\"%-%\"))\n",
    "\n",
    "window = Window.partitionBy(\"city\").orderBy(F.desc(\"hours\"))\n",
    "df_window = df_filter.withColumn(\"rank\", F.dense_rank().over(window))\n",
    "df_filter = df_window.filter(F.col(\"rank\") == 1)\n",
    "df_rename = df_filter.withColumnRenamed(\"name\", \"category\")\n",
    "df_rename.show()\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f53bfd46-e3ec-4bb1-bc5a-d6270f9eaec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b48c6945-b2c5-4d57-a9aa-2034945a91f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d094c95-f525-4df5-a07c-856054a54470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spark)",
   "language": "python",
   "name": "spark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
